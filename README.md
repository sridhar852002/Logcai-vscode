# ğŸš€ LogCAI - Local + Cloud AI Coding Assistant

> Supercharge your VS Code with blazing-fast local AI and intelligent coding assistance.


## âœ¨ Core Features

- ğŸ§  **Chat with AI inside VS Code**  
  Get streaming, interactive chat responses from your models.

- âœï¸ **Inline Code Suggestions**  
  Autocomplete code intelligently, with live updates as you type.

- ğŸ“š **Retrieval-Augmented Generation (RAG)**  
  Automatically index your project and inject relevant snippets into AI prompts.

- ğŸ”€ **Switch Model Providers**  
  Choose between Ollama (local), OpenAI, or Anthropic anytime.

- ğŸ›¡ï¸ **Secure API Key Storage**  
  Your API keys are encrypted and stored securely using VS Code's Secret Storage.

- ğŸ“ˆ **Status Bar Connection Status**  
  Always know if your model is connected or needs attention.

- âš¡ **Streamed Completions**  
  Chat and inline code completions stream live without delay.

- ğŸ§© **Command Palette Shortcuts**  
  Easily open chat, trigger completions, manage models, run diagnostics, and more.

- ğŸ¨ **Beautiful Syntax-Highlighted Chat UI**  
  Supports copy-to-clipboard, regenerate response, and insert to editor.

- ğŸ§  **Language-Aware Context Extraction**  
  Smartly extracts local functions, classes, and imports for better AI accuracy.

- ğŸ§¹ **Project Structure Awareness**  
  Uses a file tree overview during prompt construction for smarter replies.

- ğŸš€ **Install Ollama Models Easily**  
  Directly install popular Ollama models from the extension.

---

## ğŸ› ï¸ Supported Model Providers

| Provider | Mode | Details |
|:---------|:-----|:--------|
| **Ollama** | Local | No internet needed. Models run fully offline. |
| **OpenAI** | Cloud | Supports GPT-4o, GPT-4 Turbo, etc. |
| **Anthropic** | Cloud | Supports Claude 3 Opus and others. |

Switch easily using the `LogCAI: Select Model Provider` command.

---

## ğŸ› ï¸ Available Commands

- **LogCAI: Open Chat** â€“ Open the chat panel.
- **LogCAI: Get Inline Completion** â€“ Trigger inline code generation manually.
- **LogCAI: Index Codebase** â€“ Build your workspace RAG index.
- **LogCAI: Clear Codebase Index** â€“ Reset RAG storage.
- **LogCAI: Install Ollama Model** â€“ Install missing models on the fly.
- **LogCAI: Run Diagnostics** â€“ Check server connections and model health.
- **LogCAI: Select Model Provider** â€“ Switch between Ollama, OpenAI, Anthropic.
- **LogCAI: Select Ollama Model** â€“ Pick your active Ollama model.

---

## âš™ï¸ Key Settings

| Setting | Description |
|:--------|:------------|
| `logcai.modelProvider` | Choose your model backend. |
| `logcai.temperature` | Control randomness of output. |
| `logcai.maxTokens` | Maximum response length. |
| `logcai.enableRAG` | Enable or disable project context retrieval. |
| `logcai.inlinePreviewDelay` | Delay for showing inline previews. |
| `logcai.maxFilesToProcess` | Control project indexing depth. |

All configurable via **Settings â†’ Extensions â†’ LogCAI**.

---

## ğŸ“¦ Installation

1. Install **Ollama** if using local models:  
   ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)

2. Start Ollama server:
   ```bash
   ollama serve
   ```

3. Install LogCAI from VSCode Marketplace 


4. Open the Command Palette and search `LogCAI`.

---

## ğŸ›¡ï¸ Security

- âœ… API keys stored securely in VS Code Secret Storage.
- âœ… No hidden telemetry.
- âœ… Ollama local mode keeps your data private and offline.

---

## ğŸ“œ License

This project is licensed under the [MIT License](./LICENSE).

---

## ğŸ™Œ Built With

- VS Code API
- TypeScript
- Ollama
- OpenAI
- Anthropic
- Love â¤ï¸ by [Sridhar](https://logcai.com)

---

# ğŸš€ Install LogCAI and Code Smarter â€” Locally and Privately!

---
